[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Phylogenetic tree with a geographic map",
    "section": "",
    "text": "In a recent meeting, I was shown an impressive figure and asked if I could attempt to recreate it. It was an amazing graphic, one that immediately struck me as having been created in Python. Unfortunately, I am far more proficient in R than in Python, especially when it comes to producing publication-quality figures. So I went about spedning the next two weeks in seeing if it could be done in R.\nLets take a look at our opponent:\nA few things that I imidiately got down was the"
  },
  {
    "objectID": "posts/2025-08-29_tree-with-map/index.html",
    "href": "posts/2025-08-29_tree-with-map/index.html",
    "title": "Phylogenetic tree with a geographic map",
    "section": "",
    "text": "In a recent meeting, I was shown an impressive figure and asked if I could attempt to recreate it. It was an amazing graphic, one that immediately struck me as having been created in Python. Unfortunately, I am far more proficient in R than in Python, especially when it comes to producing publication-quality figures. So I went about spending the next two weeks in seeing if it could be done in R.\nLets take a look at our opponent, Figure 2 from the publication: “Emergence and spread of SARS-CoV-2 lineage B.1.620 with variant of concern-like mutations and deletions”. Im not really sure how copyrights work,so I am not going to include the image here, but I made a mock-up of what I want to achieve.\nWe have a circular maximum likelihood tree with projections onto a map of Europe. The tree’s tip points are colored according to country, with lines extending from each tip to an “inner circle”, from which they continue outward to the corresponding geographic coordinates. I like the use of this inner circle, it prevent the connective lines from crossing directly across the tree, retaining the structure of the branches.\nThere are a few obvious problems that struck me as I set out on this endeavor, the first being that circular trees will have polar coordinates (originating from the tree root), and the map with have cartographic coordinates. While I have not really considered the two together before, it seems obvious that they would not mix well.\nThe first thing I did was check to see if this publication has any code avaialble, and they did. The authors of this really neat paper detailed their analysis extremely well and can be found on their GitHub page.\nIt appears that following is going on in this code:\nIm sure it’ll be ten nice and easy steps."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Anatomy of a BASH",
    "section": "",
    "text": "In this blog, I share a number of tips and tricks to help you write BASH scripts for microbiological data analysis."
  },
  {
    "objectID": "posts/2025-08-29_tree-with-map/index.html#preparing-some-test-data",
    "href": "posts/2025-08-29_tree-with-map/index.html#preparing-some-test-data",
    "title": "Phylogenetic tree with a geographic map",
    "section": "Preparing some test data",
    "text": "Preparing some test data\n\nPhylogenetic tree\nCreating a test phylogenetic tree is easily achieved as such using ggtree, for which the resources are avaialble here.\n\n# echo: true\n# eval: true\n# warning: false\n# load library\nlibrary(ggtree)\n\nggtree v3.16.3 Learn more at https://yulab-smu.top/contribution-tree-data/\n\nPlease cite:\n\nGuangchuang Yu, Tommy Tsan-Yuk Lam, Huachen Zhu, Yi Guan. Two methods\nfor mapping and visualizing associated data on phylogeny using ggtree.\nMolecular Biology and Evolution. 2018, 35(12):3041-3043.\ndoi:10.1093/molbev/msy194\n\n# set random seed\nset.seed(1234)\n\n# create a test tree and plot\ntree &lt;- rtree(50)\ntree_p &lt;- ggtree(tree) + \n    layout_inward_circular(xlim=15)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\ntree_p"
  },
  {
    "objectID": "posts/2025-08-29_tree-with-map/index.html#preparing-test-data",
    "href": "posts/2025-08-29_tree-with-map/index.html#preparing-test-data",
    "title": "Phylogenetic tree with a geographic map",
    "section": "Preparing test data",
    "text": "Preparing test data\n\nPhylogenetic tree\nCreating a test phylogenetic tree is easily achieved as such using ggtree, for which the resources are avaialble here.\n\n# echo: true\n# eval: true\n# warning: false\n# load library\nlibrary(ggtree)\n\nggtree v3.16.3 Learn more at https://yulab-smu.top/contribution-tree-data/\n\nPlease cite:\n\nGuangchuang Yu, Tommy Tsan-Yuk Lam, Huachen Zhu, Yi Guan. Two methods\nfor mapping and visualizing associated data on phylogeny using ggtree.\nMolecular Biology and Evolution. 2018, 35(12):3041-3043.\ndoi:10.1093/molbev/msy194\n\n# set random seed\nset.seed(1234)\n\n# create a test tree and plot\ntree &lt;- rtree(50)\ntree_p &lt;- ggtree(tree) + \n    layout_inward_circular(xlim=15)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\ntree_p"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BASHing my head with code",
    "section": "",
    "text": "Part 3:\n\n\n\ncode\n\nanalysis\n\nbeginners\n\nAnatomy of a BASH\n\nGuide\n\n\n\n\n\n\n\n\n\nAug 28, 2025\n\n\nPoppy J Hesketh Best\n\n\n\n\n\n\n\n\n\n\n\n\nPart 4: Making your script pretty.\n\n\n\ncode\n\nanalysis\n\nbeginners\n\nAnatomy of a BASH\n\nGuide\n\n\n\n\n\n\n\n\n\nAug 28, 2025\n\n\nPoppy J Hesketh Best\n\n\n\n\n\n\n\n\n\n\n\n\nPart 5: Writing smarter scripts.\n\n\n\ncode\n\nanalysis\n\nbeginners\n\nAnatomy of a BASH\n\nGuide\n\n\n\n\n\n\n\n\n\nAug 28, 2025\n\n\nPoppy J Hesketh Best\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2: Starting your script with a bang\n\n\n\ncode\n\nanalysis\n\nbeginners\n\nAnatomy of a BASH\n\nGuide\n\n\n\n\n\n\n\n\n\nAug 28, 2025\n\n\nPoppy J Hesketh Best\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Writing Modular Scripts\n\n\n\ncode\n\nanalysis\n\nbeginners\n\nAnatomy of a BASH\n\nGuide\n\n\n\n\n\n\n\n\n\nAug 28, 2025\n\n\nPoppy J Hesketh Best\n\n\n\n\n\n\n\n\n\n\n\n\nPart 6: Examples of modules scripts.\n\n\n\ncode\n\nanalysis\n\nbeginners\n\nAnatomy of a BASH\n\nGuide\n\n\n\n\n\n\n\n\n\nAug 28, 2025\n\n\nPoppy J Hesketh Best\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/intro-to-shell/index.html",
    "href": "posts/intro-to-shell/index.html",
    "title": "Part 1: Writing Modular Scripts",
    "section": "",
    "text": "Why should you be writing modular scripts? How can you get started with them?\n⮕ Part 2"
  },
  {
    "objectID": "posts/intro-to-shell/index.html#why-write-modular-scripts",
    "href": "posts/intro-to-shell/index.html#why-write-modular-scripts",
    "title": "Part 1: Writing Modular Scripts",
    "section": "Why write modular scripts?",
    "text": "Why write modular scripts?\nFor a long time, my bioinformatics analyses were slow and laborious. This was mostly because my scripts were built specifically for a particular sample (or set of samples). This approach likely stems from the way most bioinformatics courses and tutorials are taught—running a single sample through a process at a time, then using loops to parse multiple samples.\nThe problem is that once the amount of data scales up, over-relying on loops to process every sample at each step can make scripts painfully slow and limiting. Paths are often hard-coded to a particular directory, variables may be specific to a single server, and so on. While this is a common way to handle genomic analysis and data processing, it quickly falls apart when you move institutes, share your scripts, or try to make your work publicly available.\nBASH isn’t always the best language for modular, scalable workflows (Python is a strong alternative), but if you’re comfortable with a little BASH and haven’t had time to dive into Python yet, this guide is for you. It will help you start writing multi-sample scripts that can be submitted to a server, generate data consistently, and be easily shared with colleagues—or deposited directly into project repositories for reviewers and readers to use."
  },
  {
    "objectID": "posts/intro-to-shell/index.html#aims",
    "href": "posts/intro-to-shell/index.html#aims",
    "title": "Part 1: Writing Modular Scripts",
    "section": "Aims:",
    "text": "Aims:\nThe main aim of this series of posts is to show you ways to write BASH, R and Python scripts that:\n\nPerform single/limited function(s) - suitable for any appropriate data-type for that function/programme (modular scripts)\nUsing data located in any part of the computer/server\nProducing outputs in a default or specified location/name\nHas useful help messages\nProduces usefull error mesages\nCan be submited to any server type (e.g. SunCluter, SLURM)."
  },
  {
    "objectID": "posts/intro-to-shell/index.html#definitions",
    "href": "posts/intro-to-shell/index.html#definitions",
    "title": "Part 1: Writing Modular Scripts",
    "section": "Definitions",
    "text": "Definitions\nMany of these terms and phrases should already be familiar if you’ve been running commands in a terminal or submitting jobs to a high-performance computer. Some of the terms are my own descriptions — they might not be universally recognized, but they reflect how I think about them (see Table 1).\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nFunctions\nA Bash function is a block of reusable code designed to perform a particular operation. Once defined, the function can be called multiple times within a script.\n\n\nHPC\nHigh performing computers\n\n\nLoops: for\nA BASH for loop is a programming language statement which allows code to be repeatedly executed for a selection of data (i.e. for file in read/*R1_fastq.gz; do | meaning for every R1 file in the reads directory, perform the following)\n\n\nLoops: until\nA BASH until loop is executes a code block repeatedly until a specific condition becomes true.\n\n\nLoops: while\nPerform an action while an expression is true, keep executing these lines of code.\n\n\nModular scripts\nScripts that are not limited by location or data, and can be chained by a workflow manager\n\n\nHPS: Modules\nThe module system is a concept available on most supercomputers, simplifying the use of different software (versions) in a precise and controlled manner.\n\n\nPipeline\nA pipeline is a series of processes, usually linear, which filter or transform data. The processes are generally assumed to be running concurrently. The first process takes raw data as input, does something to it, then sends its results to the second process, and so on, eventually ending with the final result being produced by the last process in the pipeline. Pipelines are normally quick, with a flow taking seconds to hours for end-to-end processing of a single set of data.\n\n\nStatements: elif\nThe elif statement, stands for “else if”. It can be used in conditional statements to check for multiple conditions. For example, if the first condition is false, it moves on to the next “elif” statement to check if that condition is true.\n\n\nStatements: else\nAn else statement pecifies that alternate processing is to take place when the conditions of the matching IF statement are not satisfied\n\n\nStatements: if\nAn if statement is a programming construct that allows you to make decisions based on certain conditions. It helps you control the flow of your program by executing different blocks of code depending on whether a given condition is true or false.\n\n\nVariable\nA BASH variable acts as temporary storage for a string or a number. Variables also make it easy for users to write complex functions and perform various operations. Users can create variables by giving them a name and a value. A name can be anything. (e.g. ${input})\n\n\nWorkflow\na set of processes, usually non-linear, often human rather than machine, which filter or transform data, often triggering external events. The processes are not assumed to be running concurrently. The data flow diagram of a pipeline can branch or loop. There may be no clearly defined “first” process – data may enter the workflow from multiple sources. Any process may take raw data as input, do something to it, then send its results to another process\n\n\n\n\nModular scripts\nSince most scripts are sent to a computing cluster, they generally need to be written as BASH scripts (some servers do accept Python, though I’ve had issues with activating Conda in the past). For simple local tasks—like using seqkit to check the number of reads in a FASTQ or FASTA file — I often just run commands directly. But for more complex processes, such as genome assembly and quality-checking contigs, I write scripts that I can submit to the cluster.\nI find it really helpful to approach script writing with modularity in mind (Figure 1). This means breaking down a workflow into individual functions or steps, figuring out the inputs and outputs at each step or for each tool, and designing it so that data from any project can be fed in, with the outputs ready for the next step.\nTaking this modular approach also makes it easier to combine multiple scripts into a larger pipeline using workflow managers like NextFlow or Snakemake. This allows you to automate an entire process, for example:\nraw FASTQ → adaptor trimming and QC → assembly → contig QC → annotation → phylogeny\nYou don’t always need to write your own pipeline — there are plenty of tools available for bacterial and viral genomics. But if you’re working with a non-model organism, or need to use tools specifically tailored to a particular species, of there is an analysis you expect to be repeating with any regularity, building your own modular workflows can save a lot of time and make your analyses more reproducible and shareable with colleagues.\n\n\n\nFigure 1. Workflow example of a genome assembly composed of five scripts (modules, blue) that performs individual analysis and are chained by the wrapper script (green)\n\n\nI like to think of scripts as often composed of three parts, a set-up, the main, and the closing. The set up for the script will include things such as defining argument flags, parameters, opening virtual environments (if needed), help function, setting up a log output. The main body will compose of the primary purpose of the scripts, to run a specific or series of programmes, and the closing of the script will include anything got wrap it all up.\nThis tutorial will mostly discuss the setup of a script, as that is where you can refine your script to be modular. In the example bellow you can see highlighted the what that setup might look like (Figure 2.).\n\n\n\nFigure 2. Example script that downloads from NCBI the assembly data for bacterial genomes and accepts a list of bacterial genome accession IDs to download either the nucleotides or proteins. Highlighted is the part of the script that will be covered in this guide.\n\n\nPart 2:"
  },
  {
    "objectID": "posts/part2/index.html",
    "href": "posts/part2/index.html",
    "title": "Part 2: Starting your script with a bang",
    "section": "",
    "text": "Here we cover creating help messages,\nPart 1 ⋘ Part 2 ⋙ Part 3"
  },
  {
    "objectID": "posts/part1/index.html",
    "href": "posts/part1/index.html",
    "title": "Part 1: Writing Modular Scripts",
    "section": "",
    "text": "Why should you be writing modular scripts? How can you get started with them?\n⮕ Part 2"
  },
  {
    "objectID": "posts/part1/index.html#why-write-modular-scripts",
    "href": "posts/part1/index.html#why-write-modular-scripts",
    "title": "Part 1: Writing Modular Scripts",
    "section": "Why write modular scripts?",
    "text": "Why write modular scripts?\nFor a long time, my bioinformatics analyses were slow and laborious. This was mostly because my scripts were built specifically for a particular sample (or set of samples). This approach likely stems from the way most bioinformatics courses and tutorials are taught—running a single sample through a process at a time, then using loops to parse multiple samples.\nThe problem is that once the amount of data scales up, over-relying on loops to process every sample at each step can make scripts painfully slow and limiting. Paths are often hard-coded to a particular directory, variables may be specific to a single server, and so on. While this is a common way to handle genomic analysis and data processing, it quickly falls apart when you move institutes, share your scripts, or try to make your work publicly available.\nBASH isn’t always the best language for modular, scalable workflows (Python is a strong alternative), but if you’re comfortable with a little BASH and haven’t had time to dive into Python yet, this guide is for you. It will help you start writing multi-sample scripts that can be submitted to a server, generate data consistently, and be easily shared with colleagues—or deposited directly into project repositories for reviewers and readers to use."
  },
  {
    "objectID": "posts/part1/index.html#aims",
    "href": "posts/part1/index.html#aims",
    "title": "Part 1: Writing Modular Scripts",
    "section": "Aims:",
    "text": "Aims:\nThe main aim of this series of posts is to show you ways to write BASH, R and Python scripts that:\n\nPerform single/limited function(s) - suitable for any appropriate data-type for that function/programme (modular scripts)\nUsing data located in any part of the computer/server\nProducing outputs in a default or specified location/name\nHas useful help messages\nProduces usefull error mesages\nCan be submited to any server type (e.g. SunCluter, SLURM)."
  },
  {
    "objectID": "posts/part1/index.html#definitions",
    "href": "posts/part1/index.html#definitions",
    "title": "Part 1: Writing Modular Scripts",
    "section": "Definitions",
    "text": "Definitions\nMany of these terms and phrases should already be familiar if you’ve been running commands in a terminal or submitting jobs to a high-performance computer. Some of the terms are my own descriptions — they might not be universally recognized, but they reflect how I think about them (see Table 1).\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nFunctions\nA Bash function is a block of reusable code designed to perform a particular operation. Once defined, the function can be called multiple times within a script.\n\n\nHPC\nHigh performing computers\n\n\nLoops: for\nA BASH for loop is a programming language statement which allows code to be repeatedly executed for a selection of data (i.e. for file in read/*R1_fastq.gz; do | meaning for every R1 file in the reads directory, perform the following)\n\n\nLoops: until\nA BASH until loop is executes a code block repeatedly until a specific condition becomes true.\n\n\nLoops: while\nPerform an action while an expression is true, keep executing these lines of code.\n\n\nModular scripts\nScripts that are not limited by location or data, and can be chained by a workflow manager\n\n\nHPS: Modules\nThe module system is a concept available on most supercomputers, simplifying the use of different software (versions) in a precise and controlled manner.\n\n\nPipeline\nA pipeline is a series of processes, usually linear, which filter or transform data. The processes are generally assumed to be running concurrently. The first process takes raw data as input, does something to it, then sends its results to the second process, and so on, eventually ending with the final result being produced by the last process in the pipeline. Pipelines are normally quick, with a flow taking seconds to hours for end-to-end processing of a single set of data.\n\n\nStatements: elif\nThe elif statement, stands for “else if”. It can be used in conditional statements to check for multiple conditions. For example, if the first condition is false, it moves on to the next “elif” statement to check if that condition is true.\n\n\nStatements: else\nAn else statement pecifies that alternate processing is to take place when the conditions of the matching IF statement are not satisfied\n\n\nStatements: if\nAn if statement is a programming construct that allows you to make decisions based on certain conditions. It helps you control the flow of your program by executing different blocks of code depending on whether a given condition is true or false.\n\n\nVariable\nA BASH variable acts as temporary storage for a string or a number. Variables also make it easy for users to write complex functions and perform various operations. Users can create variables by giving them a name and a value. A name can be anything. (e.g. ${input})\n\n\nWorkflow\na set of processes, usually non-linear, often human rather than machine, which filter or transform data, often triggering external events. The processes are not assumed to be running concurrently. The data flow diagram of a pipeline can branch or loop. There may be no clearly defined “first” process – data may enter the workflow from multiple sources. Any process may take raw data as input, do something to it, then send its results to another process\n\n\n\n\nModular scripts\nSince most scripts are sent to a computing cluster, they generally need to be written as BASH scripts (some servers do accept Python, though I’ve had issues with activating Conda in the past). For simple local tasks—like using seqkit to check the number of reads in a FASTQ or FASTA file — I often just run commands directly. But for more complex processes, such as genome assembly and quality-checking contigs, I write scripts that I can submit to the cluster.\nI find it really helpful to approach script writing with modularity in mind (Figure 1). This means breaking down a workflow into individual functions or steps, figuring out the inputs and outputs at each step or for each tool, and designing it so that data from any project can be fed in, with the outputs ready for the next step.\nTaking this modular approach also makes it easier to combine multiple scripts into a larger pipeline using workflow managers like NextFlow or Snakemake. This allows you to automate an entire process, for example:\nraw FASTQ → adaptor trimming and QC → assembly → contig QC → annotation → phylogeny\nYou don’t always need to write your own pipeline — there are plenty of tools available for bacterial and viral genomics. But if you’re working with a non-model organism, or need to use tools specifically tailored to a particular species, of there is an analysis you expect to be repeating with any regularity, building your own modular workflows can save a lot of time and make your analyses more reproducible and shareable with colleagues.\n\n\n\nFigure 1. Workflow example of a genome assembly composed of five scripts (modules, blue) that performs individual analysis and are chained by the wrapper script (green)\n\n\nI like to think of scripts as often composed of three parts, a set-up, the main, and the closing. The set up for the script will include things such as defining argument flags, parameters, opening virtual environments (if needed), help function, setting up a log output. The main body will compose of the primary purpose of the scripts, to run a specific or series of programmes, and the closing of the script will include anything got wrap it all up.\nThis tutorial will mostly discuss the setup of a script, as that is where you can refine your script to be modular. In the example bellow you can see highlighted the what that setup might look like (Figure 2.).\n\n\n\nFigure 2. Example script that downloads from NCBI the assembly data for bacterial genomes and accepts a list of bacterial genome accession IDs to download either the nucleotides or proteins. Highlighted is the part of the script that will be covered in this guide.\n\n\nPart 2:"
  },
  {
    "objectID": "posts/part2/index.html#printing-a-help-message",
    "href": "posts/part2/index.html#printing-a-help-message",
    "title": "Part 2: Starting your script with a bang",
    "section": "Printing a help message",
    "text": "Printing a help message\nA help message is just a chunk of text, and unlike in R and Python is actually detached from the argument definement (covered below). This means, that just because you write it in your help message, does not actually mean it is reflected in the defined arguments. Because of this, I prefer to have the help message written before writing the argument definition, as it helps remind me exactly what I need. For this you simply define a function called Help(), and open up the function with curly brackets {. The help message itself it just generated using echo. After the message is written, you close the function with another curly bracket }. The help message can be called simply by using the new command Help, in your script.\nPosition wise this should be placed before the argument definement, as one flag will to call the help function (-h), it needs to be defined before.\n\nHelp(){\n    echo \"Help message for the script: my script\n        Required:\n            -i      path to input file\n            -o      path to output file\n        Optional:\n            -t      number of threads (default : 4)\n            -h      print this help message and exit\n}\n\n# Now you can call the Help function\nHelp"
  },
  {
    "objectID": "posts/part2/index.html#defining-arguments",
    "href": "posts/part2/index.html#defining-arguments",
    "title": "Part 2: Starting your script with a bang",
    "section": "Defining arguments",
    "text": "Defining arguments\nOne way to achieve a multi-sample functioning script very simply is to feed files into your script, rather than have the target file/values defined in the script. The most basic way to do this in BASH is to just add them to the end of you line of code:\n\n# write the example script\necho \"#!/usr/bin/env bash\neval \"$(conda shell.bash hook)\"\nconda activate spades\nspades.py -1 ${1} -2 ${2} -o ${3}\nconda deactivate\n\" &gt; assembly.sh\n\n# This script could be run as follows\nassembly.sh sample-1_R1.fastq.gz sample-1_R2.fastq.gz sample-1_assembly-out\n\nYour inputs are defined sequentially, so the first file/value following assembly.sh becomes the variable \\${1}, the second \\${2}, so on. This is not the most user-friendly, and if you are going to be sharing scripts with colleagues it can be clunky for others. This is why its valuable to write script that have help mesages and defined flags. Using the same example as above, but this time there is a input flag for the required files (R1 and R2 reads), and an output directory:\n\nassembly.sh -i sample-1_R1.fastq.gz -r sample-2_R1.fastq.gz -o sample-1_assembly-out\n\nFirst, we need to define the arguments for the script, and this is achieved using the while getopts a\\:b:c:h option; do. Which looks scarier than it actually is. This is utilising a while statement to search for input flags, before here as i\\:o:t:h, which just means it will search for -i, -o, -t, and -h. Next those flags are defined into variables, for example the input flag it is defined as such: i)input=${OPTARG};;.\n\nwhile getopts i:o:t:Q:h option\ndo \n    case \"${option}\" in \n        i)input=${OPTARG};;\n        o)output=${OPTARG};;\n        t)threads=${OPTARG};;\n        Q)QCSPAdes=false;;\n        h)Help; exit;;\n    esac\ndone\n\nNow for the rest of the script, whenever you call the variable ${input}, the script will utilised whatever file/value was provided when the script was run (e.g. -i path/to/contigs.fasta). This is done for all the flags you need for your script to operate as intended while providing flexibility. The input doesnt need to be file paths, it can be values, chunks of text.\nYou can change this to have as many or as few arguments as you want. It is important to ensure that your help message (encoded in the function Help as described above), is as written to reflect the contents of the while argument. When the help message is called (e.g. assembly.sh -h), the script will terminate immediately after printing the help message, you acheive this by having the default bash function exit in h)Help; exit;;.\nA final feature you can take advatage of is utilising a true/false statement in the argument definement (e.g Q)QCSPAdes=false;;). For example, the assembler SPAdes perform an optional read error correction by default, but if you perform error correction prior you may want to disable this with the SPAdes specific flag --only-assembler. In the example script the the flag -Q is added for when you want SPAdes to only perform the assembly, or omit the flag it in order to resort to the default setting, which in the example is false (i.e. perform/do not disable error correction). If you do this, you will later have to include an if statement in order to enact this in both a true and false scenario (this will be covered later)."
  },
  {
    "objectID": "posts/part2/index.html#producing-error-messages",
    "href": "posts/part2/index.html#producing-error-messages",
    "title": "Part 2: Starting your script with a bang",
    "section": "Producing error messages",
    "text": "Producing error messages\nWhen you begin incorporating flags, its important that you set up some error messages to help people troubleshoot why the script might not be working for them. As by default, the error messages produced by BASH might not be the most informative to someone unfamiliar with the language.\nRecall in the help message there where two types of input flags, required and optional. Lets go about creating error messages based on those two categories, starting with the required arguemnts. In a scenario where a user did not define a required arguments, we need to inform users what they have missed. We will do this with if statements.\n\nif [[ -z ${inpput} ]]; then echo -e \"ERROR: -i, input is missing\"; Help, exit 1; fi\n\nThe logic of this if statement, is in the situation where -i is not defined - the -z string - which means that the if statement is true if the string is empty (i.e. human language: \\${input} does not exist/is empty because -i was not provided), then the following action is performed. That action is to print a message (\"ERROR: -i, input is missing\"), then call the Help function, which will print the help message to remind the user what the flags of the script are, then close the script with exit 1. If the statemet is not true (i.e human language: \\${input} is not empty, before a input was provided), the if statement ends (fi, for finish), and the script continues. There are additional features to if statements that will be covered later, but they are very powerful tools to utilise in your script.\nFor optional argument, we don’t need to colapse the script if no file or value was provide when the scrip was run, instead default value/path can be utilised. In the help message, we stated that the default number of threads was 4. So we set it as such using another if statement.\n\nif [[ -z \"${threads}\" ]]; then threads=4; fi\n\nThe same logic as before is applied, except this time we set the threads variable to the default value before ending the if statement (threads=4; fi). Now in the script, unless a -t value was given, whenever ${threads} is called the default value will be used. You can even add more to this statement, to include a message to inform the user that the default value has been applied.\nHave a go and write the following if statements:\n\nFor the optional -t flag, add a message that the number of threads is the default value.\nAn error message for the scenario where -o has not been provided\nMake the -o an optional flag and default the output directory to the current working directory\n\n\n\n\n\n\n\nSolution to 1\n\n\n\n\n\nSince ${threads} has a value, it is useful to use the variable instead of a hard number, as at some point in the future you might change the value of the default, and you want to have a script where you need to change that value in as few places as possible. In the present example, you would only have to change that value in two places: the if statement and the Help message.\n\nif [[ -z \"${threads}\" ]]; then \n  threads=4; \n  echo \"Number of threads not specified, utilising default value ${threads}\nfi\n\n\n\n\n\n\n\n\n\n\nSolution to 2\n\n\n\n\n\nSimply need to add a echo to the if statement. This can be personalised hower you want as long as it is informative to the user.\n\nif [[ -z ${output} ]]; then echo -e \"ERROR: -o, output is missing\"\n  Help, exit 1\nfi\n\n\n\n\n\n\n\n\n\n\nSolution to 3\n\n\n\n\n\nTo define the output as the working directory you will need to use the function $(pwd), which print the full path to the curren working directory. That is how we define the output. Same as in (1), we will use the variable ${output} to report the output path.\n\nif [[ -z ${output} ]]; then \n  output=$(pwd)\nfi\n\n# Print message to inform of the working directory\necho -e \"Output path is missing, defaulting to current directory: ${output}\""
  },
  {
    "objectID": "posts/part3/index.html",
    "href": "posts/part3/index.html",
    "title": "Part 3:",
    "section": "",
    "text": "Here we cover generating log files, and environment set up.\nPart 2 ⋘ Part 3 ⋙ Part 4"
  },
  {
    "objectID": "posts/part3/index.html#generating-logs-files",
    "href": "posts/part3/index.html#generating-logs-files",
    "title": "Part 3:",
    "section": "Generating logs-files",
    "text": "Generating logs-files\nLog files are invaluable for troubleshooting your scripts, or having a record of every run of your script (think of them as a lab book entry). You might not always keep all logs, but they are useful for looking back and remining yourself of what parameters the script was run using.\nFirst we want to create an accrate time-point in the format of YYYYMMDDHHMMSS, this might seem excessive, but considering you could be running this script multiple time making small adjustments, you want each logfile to be completely unique to not overwrite them. To achieve that we create a variable that contains the log file name. First create a variable called ${logfile}, we use the default BASH function date, and specifiy the date format '+%Y%m%d%H%M%S'. We then add the suffix .log to the file name.\n\nlogfile=\"$(date '+%Y%m%d%H%M%S')\".log\n\nNow with the logfile name defined as a variable (${logfile}), we redirect both standard output (stdout) and standard error (stderr) to a log file, while also displaying them on the terminal at the same time. Let’s break it down step by step:\n\nexec &gt; &gt;( tee -a \"${logfile}\" ) 2&gt;&1\n\n\nexec changes the behavior of stdout and stderr for the rest of the script,\nstdout (&gt;) is redirected into a process substitution: &gt;( tee -a \"${logfile}\" ). This sends the output to the tee command.\ntee displays the output on the terminal (as usual) and appends it to the file ${logfile}.\n2&gt;&1 redirects stderr to the same destination as stdout, meaning that errors are also both displayed on the terminal and written to the log file.\n\n\nlogfile=\"$(date '+%Y%m%d%H%M%S')\".log\nexec &gt; &gt;( tee -a \"${logfile}\" ) 2&gt;&1\n\nWith that you will generate a output containing the standard outputs and the standard errors, which is great for troubleshooting your code. If you are routinely sending scripts to a HPC, the submission manager of that HPC more than likely generates log files, so you might not actually need this unless you want to hoard log files."
  },
  {
    "objectID": "posts/part3/index.html#making-a-pretty-script",
    "href": "posts/part3/index.html#making-a-pretty-script",
    "title": "Part 3:",
    "section": "Making a pretty script!",
    "text": "Making a pretty script!\nIn the long matrix stream of text that can be printed to you terminal, it is often useful to have important text stand out. This could be a missing file, non-existent paths, location of output file…\nAdding color to you script can be definted very easily. First you must indicate the color of the text needs to change using: \\033[. This open up all subsequent text to a color change, BUT no color has been defined yet. To define a color your must add the approporiate ANSI escape codes (Table 1). For red text you would define it as: \\033[31m. All text that follows this will be green. To revert the color back to default (typically white in a console), you need to close the escape code with a \\033[m.\nTable 1. ANSI color codes and their corresponding colors.\n\n\n\ncode\ncolor\n\n\n\n\n31m\nred\n\n\n32m\ngreen\n\n\n36m\ncyan\n\n\n36m\npurple\n\n\n34m\nblue\n\n\n33m\norange\n\n\n\nA complete example might look something like this (if you are printing text to the console using the command echo, then you also have to add -e to enable interpretation of backslash escapes):\n\necho -e \"\\033[31m ERROR: something has gone very wrong because the text is all red!\\033[m\"\n\nThis is quite a awkward to add and you can very easily miss a closure, and the the entirely of your console is colored red and you’ll feel like you have made a terrible bloody mistake. A work around is to define the colors as a variables at the start of your script:\n\n# Define colors\ngreen='\\033[32m'; red='\\033[31m'; cyan='\\033[36m'; purple='\\033[35m'; nocolor='\\033[m'\n\nThis alows you to change the text color as follows:\n\necho -e \"${red}ERROR: something has gone very wrong because the text is all red!${nocolor}\""
  },
  {
    "objectID": "posts/part3/index.html#adding-a-timestamp-to-the-end-of-your-script",
    "href": "posts/part3/index.html#adding-a-timestamp-to-the-end-of-your-script",
    "title": "Part 3:",
    "section": "Adding a timestamp to the end of your script",
    "text": "Adding a timestamp to the end of your script\nIf your optimising your scripts and trying to get an idea of run-time and computing resources its useful to print out the run time. This is easily done by first creating a timestamp at the start and the end of your script, the calculating the elapsed time, then printing the time.\n\n#!/usr/bin/env bash\nstart_time=$(date +%s) # define start time\n\n#··············································#\n#··········· The rest of the script ···········#\n#··············································#\n\nfinish_time=$(date +%s) # define end time\n\n# Calculate the difference in seconds\nelapsed_time=$((finish_time - start_time))\n\n# Convert elapsed time to hours, minutes, and seconds\n((sec=elapsed_time%60, elapsed_time/=60, min=elapsed_time%60, hrs=elapsed_time/60))\ntimestamp=$(printf \"Total time taken - %d hours, %d minutes, and %d seconds.\" $hrs $min $sec)\necho $timestamp\n\n# Print the total runtime\necho -e \"\"\necho -e \"\"Total time taken: ${hrs}:${min}:${sec}\"\n\nIf you are testing multiple computing resource allocation and different processing time, you might even consider printing a running table of your experiment along with your run parameters. For example, in the case of a a phylogeny script (phylogeny)\n\necho -e \"phylogeny.sh;${THREADS};${NBOOTSTRAPS};${NUMSEQUENCES};${hrs}:${min}:${sec}\" &gt;&gt; computing-time-test.csv"
  },
  {
    "objectID": "posts/part3/index.html#truefalse-scenarios",
    "href": "posts/part3/index.html#truefalse-scenarios",
    "title": "Part 3:",
    "section": "true/false scenarios:",
    "text": "true/false scenarios:\nPreviously the idea of having a true/false argument for dissabling the error correction of SPAdes, as an example. If you have included an argument for when QCSPADes == true and when QCSPADes == false.\nLike before we start the if statement with a scenario, which is QCSPADes == true (for when the -Q flag is uses), then we direct a specific action for that scenario, which is to performs SPADes using the assembly only features disabling the error correction.\n\n\nif [[ QCSPADes == true ]]; then \n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out --only-assembler\n···\n\nThen we can continue the if statement to include an alternative scenario using an ‘else if’ (elif) statement, for for scenario when QCSPADes == false, to perform spades with default features. This is also the default functioning of the example script as we defined Q)QCSPADes=false;; at the start of the script.\n\n···\nelif [[ QCSPADes == false ]]; then\n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out\nfi\n\nThe entire if argument will look as follows:\n\nif [[ QCSPADes == true ]]; then \n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out --only-assembler\nelif [[ QCSPADes == false ]]; then\n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out\nfi"
  },
  {
    "objectID": "posts/part3/index.html#not-running-the-script-if-the-output-of-the-tool-already-exists",
    "href": "posts/part3/index.html#not-running-the-script-if-the-output-of-the-tool-already-exists",
    "title": "Part 3:",
    "section": "Not running the script if the output of the tool already exists:",
    "text": "Not running the script if the output of the tool already exists:\nIn the example bin/tb-profiler_v1.sh you can see that there is an example to search a collated output from previous runs of this scrip before deciing to run the script or not. It uses an if statement to determine wether that particular sampleID exists the output, and if it does not (i.e. ) to run the script. But if the output does exists (i.e. ), the it skips that particuler sample."
  },
  {
    "objectID": "posts/part3/index.html#forcing-the-script-to-run-even-if-the-output-exists",
    "href": "posts/part3/index.html#forcing-the-script-to-run-even-if-the-output-exists",
    "title": "Part 3:",
    "section": "Forcing the script to run even if the output exists:",
    "text": "Forcing the script to run even if the output exists:\nIn a more sophisticated version of the script bin/tb-profiler_v2.sh there is an additional flag to force the script to overright the previous output (i.e. ) using a flag of -F."
  },
  {
    "objectID": "posts/part3/index.html#adding-a-counter-if-processing-multiple-samples",
    "href": "posts/part3/index.html#adding-a-counter-if-processing-multiple-samples",
    "title": "Part 3:",
    "section": "Adding a counter if processing multiple samples",
    "text": "Adding a counter if processing multiple samples\nIf a single process is quick and not computationaly demanding, then loops are a good way to parse through lots of samples at once. I like to add a counter, so that I can keep track of where the script is at when I send it to a HPC.\nTo add a counter, you first must set the counter at one: COUNTER=1, then you will want to get the total number of samples to be processed: TOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l). With those now defined you can start the loop. In this example the loop is utilising the path ${DIRECTORY} and searching for files containing the suffix \\*_R1.fastq.gz. This is done to capture the sample ID by using basename to remove the path and suffic of the R1 file.\n\nCOUNTER=0\nTOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l); COUNTER=1\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n···\n\nWith the loop open, we can calculate the remaining number of samples to process: REMAINING=$((TOTAL - COUNTER)). Its important to know that shell can only perform simple mathematics, so bear this in mind when using its calculator functions. Then an echo -e is used to report which sample number the loop is on, and how many are remaining:\n\n···\necho -e \"   Sample: ${ID}  [${COUNTER}/${TOTAL}; ${REMAINING} remaining]\n                R1: ${DIRECTORY}/${ID}_R1.fastq.gz\n                R2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n···\n\nIn this example a tool called TB-Profiler is running on the R1 and R2 FASTQ files, utilising the \\${ID} variable defined at the start of the loop for each sample. With the main function defined, we have to remember to increase the counter by 1, so that it increases with the loop to the next file: COUNTER=$((COUNTER + 1)).\nIn all this might look something like this:\n\nCOUNTER=1 # start the counter\nTOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l) # get the total\n\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n    ID=$(basename \"${file}\" _R1.fastq.gz)\n    \n    REMAINING=$((TOTAL - COUNTER)) # Calculate remaining samples\n\n    # Display sample information with the counter\n    echo -e \"   Sample: ${ID}  [${COUNTER}/${TOTAL}; ${REMAINING} remaining]\n                R1: ${DIRECTORY}/${ID}_R1.fastq.gz\n                R2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n\n    # Run the profiling command\n    tb-profiler profile -1 ${DIRECTORY}/${ID}_R1.fastq.gz \\\n                        -2 ${DIRECTORY}/${ID}_R2.fastq.gz \\\n                        -t 4 -p ${ID} --txt\n        \n    COUNTER=$((COUNTER + 1)) # Increment the counter\ndone\n\nYou can even combine the loop with the if statement:\n\nCOUNTER=0 # start the counter at zero\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n    ID=$(basename \"${file}\" _R1.fastq.gz)\n    # Calculate remaining samples\n    REMAINING=$((TOTAL - COUNTER))\n\n    # If argument to check that the TB_profile hasnt already been run:\n    if [[ ! -f ${TBPROF_DIR}/results/${ID}.results.txt ]]; then\n        # Display sample information with the counter\n        echo -e \"${cyan}\\tSample: ${ID}\\t\\t[$COUNTER/$TOTAL; $REMAINING remaining]\\n\\t\\tR1: ${DIRECTORY}/${ID}_R1.fastq.gz\\n\\t\\tR2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n        echo -e \"${nocolor}\"\n        # Run the profiling command\n        tb-profiler profile -1 ${DIRECTORY}/${ID}_R1.fastq.gz -2 ${DIRECTORY}/${ID}_R2.fastq.gz -t ${NTHREADS} -p ${ID} --txt\n        # Increment the counter\n        COUNTER=$((COUNTER + 1))\n    elif [[ -f ${TBPROF_DIR}/results/${ID}.results.txt ]]; then\n        echo -e \"${red}\\t${TBPROF_DIR}/results/${ID}.results.txt exists, skipping: ${ID}\\t\\t[$COUNTER/$TOTAL; $REMAINING remaining]\"\n        COUNTER=$((COUNTER + 1))\n    fi\ndone"
  },
  {
    "objectID": "posts/part3/index.html#submitting-modular-scripts-to-hcp",
    "href": "posts/part3/index.html#submitting-modular-scripts-to-hcp",
    "title": "Part 3:",
    "section": "Submitting modular scripts to HCP",
    "text": "Submitting modular scripts to HCP\nDepending on your computing cluster server, you will be able to directly submit these scripts given that you provide sufficient information in your code to run with the necessary computing resources.\nSunGrid system For a SunGrid system you have to use qsub as your main command for submitting scripts. Because of how these scripts were writen, there are not cluster specific parameters defined within the script, so they can be defined with the script submission command\n\nqsub"
  },
  {
    "objectID": "posts/part3/index.html#example-scripts",
    "href": "posts/part3/index.html#example-scripts",
    "title": "Part 3:",
    "section": "Example scripts",
    "text": "Example scripts\nHere are some examples of scripts, you can also find these scripts in in bin/. Take note how these scripts make use of all that has been discussed in this guide \n\n\n\nimage\n\n\n\nPart 2 ⋘ Part 3 ⋙ Part 4"
  },
  {
    "objectID": "posts/part6/index.html",
    "href": "posts/part6/index.html",
    "title": "Part 6: Examples of modules scripts.",
    "section": "",
    "text": "Here we go over some examples of scripts and how they are modules and interesting things that they do to keep your analyses working smoothly and informative.\n**Part5 ⋘ Part 6\n\n\nExample scripts\nHere are some examples of scripts, you can also find these scripts in in bin/. Take note how these scripts make use of all that has been discussed in this guide.\n\n\n\nimage\n\n\n\n\n\nimage\n\n\n\n**Part5 ⋘ Part 6"
  },
  {
    "objectID": "posts/part4/index.html",
    "href": "posts/part4/index.html",
    "title": "Part 4: Making your script pretty.",
    "section": "",
    "text": "Here we cover generating log files, and environment set up.\nPart 2 ⋘ Part 3 ⋙ Part 4"
  },
  {
    "objectID": "posts/part4/index.html#making-a-pretty-script",
    "href": "posts/part4/index.html#making-a-pretty-script",
    "title": "Part 4: Making your script pretty.",
    "section": "Making a pretty script!",
    "text": "Making a pretty script!\nIn the long matrix stream of text that can be printed to you terminal, it is often useful to have important text stand out. This could be a missing file, non-existent paths, location of output file…\nAdding color to you script can be defined very easily. First you must indicate the color of the text needs to change using: \\033[. This open up all subsequent text to a color change, BUT no color has been defined yet. To define a color your must add the approporiate ANSI escape codes (Table 1). For red text you would define it as: \\033[31m. All text that follows this will be green. To revert the color back to default (typically white in a console), you need to close the escape code with a \\033[m.\nTable 1. ANSI color codes and their corresponding colors.\n\n\n\ncode\ncolor\n\n\n\n\n31m\nred\n\n\n32m\ngreen\n\n\n36m\ncyan\n\n\n36m\npurple\n\n\n34m\nblue\n\n\n33m\norange\n\n\n\nA complete example might look something like this (if you are printing text to the console using the command echo, then you also have to add -e to enable interpretation of backslash escapes):\n\necho -e \"\\033[31m ERROR: something has gone very wrong because the text is all red!\\033[m\"\n\nThis is quite a awkward to add and you can very easily miss a closure, and the the entirely of your console is colored red and you’ll feel like you have made a terrible bloody mistake. A work around is to define the colors as a variables at the start of your script:\n\n# Define colors\ngreen='\\033[32m'; red='\\033[31m'; cyan='\\033[36m'; purple='\\033[35m'; nocolor='\\033[m'\n\nThis alows you to change the text color as follows:\n\necho -e \"${red}ERROR: something has gone very wrong because the text is all red!${nocolor}\"\n\n\nAdding a timestamp to the end of your script\nIf your optimising your scripts and trying to get an idea of run-time and computing resources its useful to print out the run time. This is easily done by first creating a timestamp at the start and the end of your script, the calculating the elapsed time, then printing the time.\n\n#!/usr/bin/env bash\nstart_time=$(date +%s) # define start time\n\n#··············································#\n#··········· The rest of the script ···········#\n#··············································#\n\nfinish_time=$(date +%s) # define end time\n\n# Calculate the difference in seconds\nelapsed_time=$((finish_time - start_time))\n\n# Convert elapsed time to hours, minutes, and seconds\n((sec=elapsed_time%60, elapsed_time/=60, min=elapsed_time%60, hrs=elapsed_time/60))\ntimestamp=$(printf \"Total time taken - %d hours, %d minutes, and %d seconds.\" $hrs $min $sec)\necho $timestamp\n\n# Print the total runtime\necho -e \"\"\necho -e \"\"Total time taken: ${hrs}:${min}:${sec}\"\n\nIf you are testing multiple computing resource allocation and different processing time, you might even consider printing a running table of your experiment along with your run parameters. For example, in the case of a a phylogeny script (phylogeny)\n\necho -e \"phylogeny.sh;${THREADS};${NBOOTSTRAPS};${NUMSEQUENCES};${hrs}:${min}:${sec}\" &gt;&gt; computing-time-test.csv"
  },
  {
    "objectID": "posts/part4/index.html#truefalse-scenarios",
    "href": "posts/part4/index.html#truefalse-scenarios",
    "title": "Part 4: Making your script pretty.",
    "section": "true/false scenarios:",
    "text": "true/false scenarios:\nPreviously the idea of having a true/false argument for dissabling the error correction of SPAdes, as an example. If you have included an argument for when QCSPADes == true and when QCSPADes == false.\nLike before we start the if statement with a scenario, which is QCSPADes == true (for when the -Q flag is uses), then we direct a specific action for that scenario, which is to performs SPADes using the assembly only features disabling the error correction.\n\n\nif [[ QCSPADes == true ]]; then \n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out --only-assembler\n···\n\nThen we can continue the if statement to include an alternative scenario using an ‘else if’ (elif) statement, for for scenario when QCSPADes == false, to perform spades with default features. This is also the default functioning of the example script as we defined Q)QCSPADes=false;; at the start of the script.\n\n···\nelif [[ QCSPADes == false ]]; then\n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out\nfi\n\nThe entire if argument will look as follows:\n\nif [[ QCSPADes == true ]]; then \n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out --only-assembler\nelif [[ QCSPADes == false ]]; then\n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out\nfi"
  },
  {
    "objectID": "posts/part4/index.html#not-running-the-script-if-the-output-of-the-tool-already-exists",
    "href": "posts/part4/index.html#not-running-the-script-if-the-output-of-the-tool-already-exists",
    "title": "Part 4: Making your script pretty.",
    "section": "Not running the script if the output of the tool already exists:",
    "text": "Not running the script if the output of the tool already exists:\nIn the example bin/tb-profiler_v1.sh you can see that there is an example to search a collated output from previous runs of this scrip before deciing to run the script or not. It uses an if statement to determine wether that particular sampleID exists the output, and if it does not (i.e. ) to run the script. But if the output does exists (i.e. ), the it skips that particuler sample."
  },
  {
    "objectID": "posts/part4/index.html#forcing-the-script-to-run-even-if-the-output-exists",
    "href": "posts/part4/index.html#forcing-the-script-to-run-even-if-the-output-exists",
    "title": "Part 4: Making your script pretty.",
    "section": "Forcing the script to run even if the output exists:",
    "text": "Forcing the script to run even if the output exists:\nIn a more sophisticated version of the script bin/tb-profiler_v2.sh there is an additional flag to force the script to overright the previous output (i.e. ) using a flag of -F."
  },
  {
    "objectID": "posts/part4/index.html#adding-a-counter-if-processing-multiple-samples",
    "href": "posts/part4/index.html#adding-a-counter-if-processing-multiple-samples",
    "title": "Part 4: Making your script pretty.",
    "section": "Adding a counter if processing multiple samples",
    "text": "Adding a counter if processing multiple samples\nIf a single process is quick and not computationaly demanding, then loops are a good way to parse through lots of samples at once. I like to add a counter, so that I can keep track of where the script is at when I send it to a HPC.\nTo add a counter, you first must set the counter at one: COUNTER=1, then you will want to get the total number of samples to be processed: TOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l). With those now defined you can start the loop. In this example the loop is utilising the path ${DIRECTORY} and searching for files containing the suffix \\*_R1.fastq.gz. This is done to capture the sample ID by using basename to remove the path and suffic of the R1 file.\n\nCOUNTER=0\nTOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l); COUNTER=1\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n···\n\nWith the loop open, we can calculate the remaining number of samples to process: REMAINING=$((TOTAL - COUNTER)). Its important to know that shell can only perform simple mathematics, so bear this in mind when using its calculator functions. Then an echo -e is used to report which sample number the loop is on, and how many are remaining:\n\n···\necho -e \"   Sample: ${ID}  [${COUNTER}/${TOTAL}; ${REMAINING} remaining]\n                R1: ${DIRECTORY}/${ID}_R1.fastq.gz\n                R2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n···\n\nIn this example a tool called TB-Profiler is running on the R1 and R2 FASTQ files, utilising the \\${ID} variable defined at the start of the loop for each sample. With the main function defined, we have to remember to increase the counter by 1, so that it increases with the loop to the next file: COUNTER=$((COUNTER + 1)).\nIn all this might look something like this:\n\nCOUNTER=1 # start the counter\nTOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l) # get the total\n\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n    ID=$(basename \"${file}\" _R1.fastq.gz)\n    \n    REMAINING=$((TOTAL - COUNTER)) # Calculate remaining samples\n\n    # Display sample information with the counter\n    echo -e \"   Sample: ${ID}  [${COUNTER}/${TOTAL}; ${REMAINING} remaining]\n                R1: ${DIRECTORY}/${ID}_R1.fastq.gz\n                R2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n\n    # Run the profiling command\n    tb-profiler profile -1 ${DIRECTORY}/${ID}_R1.fastq.gz \\\n                        -2 ${DIRECTORY}/${ID}_R2.fastq.gz \\\n                        -t 4 -p ${ID} --txt\n        \n    COUNTER=$((COUNTER + 1)) # Increment the counter\ndone\n\nYou can even combine the loop with the if statement:\n\nCOUNTER=0 # start the counter at zero\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n    ID=$(basename \"${file}\" _R1.fastq.gz)\n    # Calculate remaining samples\n    REMAINING=$((TOTAL - COUNTER))\n\n    # If argument to check that the TB_profile hasnt already been run:\n    if [[ ! -f ${TBPROF_DIR}/results/${ID}.results.txt ]]; then\n        # Display sample information with the counter\n        echo -e \"${cyan}\\tSample: ${ID}\\t\\t[$COUNTER/$TOTAL; $REMAINING remaining]\\n\\t\\tR1: ${DIRECTORY}/${ID}_R1.fastq.gz\\n\\t\\tR2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n        echo -e \"${nocolor}\"\n        # Run the profiling command\n        tb-profiler profile -1 ${DIRECTORY}/${ID}_R1.fastq.gz -2 ${DIRECTORY}/${ID}_R2.fastq.gz -t ${NTHREADS} -p ${ID} --txt\n        # Increment the counter\n        COUNTER=$((COUNTER + 1))\n    elif [[ -f ${TBPROF_DIR}/results/${ID}.results.txt ]]; then\n        echo -e \"${red}\\t${TBPROF_DIR}/results/${ID}.results.txt exists, skipping: ${ID}\\t\\t[$COUNTER/$TOTAL; $REMAINING remaining]\"\n        COUNTER=$((COUNTER + 1))\n    fi\ndone"
  },
  {
    "objectID": "posts/part4/index.html#submitting-modular-scripts-to-hcp",
    "href": "posts/part4/index.html#submitting-modular-scripts-to-hcp",
    "title": "Part 4: Making your script pretty.",
    "section": "Submitting modular scripts to HCP",
    "text": "Submitting modular scripts to HCP\nDepending on your computing cluster server, you will be able to directly submit these scripts given that you provide sufficient information in your code to run with the necessary computing resources.\nSunGrid system For a SunGrid system you have to use qsub as your main command for submitting scripts. Because of how these scripts were writen, there are not cluster specific parameters defined within the script, so they can be defined with the script submission command\n\nqsub"
  },
  {
    "objectID": "posts/part4/index.html#example-scripts",
    "href": "posts/part4/index.html#example-scripts",
    "title": "Part 4: Making your script pretty.",
    "section": "Example scripts",
    "text": "Example scripts\nHere are some examples of scripts, you can also find these scripts in in bin/. Take note how these scripts make use of all that has been discussed in this guide \n\n\n\nimage\n\n\n\nPart 2 ⋘ Part 3 ⋙ Part 4"
  },
  {
    "objectID": "posts/part5/index.html",
    "href": "posts/part5/index.html",
    "title": "Part 5: Writing smarter scripts.",
    "section": "",
    "text": "Here we cover generating log files, and environment set up.\nPart 2 ⋘ Part 3 ⋙ Part 4"
  },
  {
    "objectID": "posts/part5/index.html#not-running-the-script-if-the-output-of-the-tool-already-exists",
    "href": "posts/part5/index.html#not-running-the-script-if-the-output-of-the-tool-already-exists",
    "title": "Part 5: Writing smarter scripts.",
    "section": "Not running the script if the output of the tool already exists:",
    "text": "Not running the script if the output of the tool already exists:\nIn the example bin/tb-profiler_v1.sh you can see that there is an example to search a collated output from previous runs of this scrip before deciing to run the script or not. It uses an if statement to determine wether that particular sampleID exists the output, and if it does not (i.e. ) to run the script. But if the output does exists (i.e. ), the it skips that particuler sample."
  },
  {
    "objectID": "posts/part5/index.html#forcing-the-script-to-run-even-if-the-output-exists",
    "href": "posts/part5/index.html#forcing-the-script-to-run-even-if-the-output-exists",
    "title": "Part 5: Writing smarter scripts.",
    "section": "Forcing the script to run even if the output exists:",
    "text": "Forcing the script to run even if the output exists:\nIn a more sophisticated version of the script bin/tb-profiler_v2.sh there is an additional flag to force the script to overright the previous output (i.e. ) using a flag of -F."
  },
  {
    "objectID": "posts/part5/index.html#adding-a-counter-if-processing-multiple-samples",
    "href": "posts/part5/index.html#adding-a-counter-if-processing-multiple-samples",
    "title": "Part 5: Writing smarter scripts.",
    "section": "Adding a counter if processing multiple samples",
    "text": "Adding a counter if processing multiple samples\nIf a single process is quick and not computationaly demanding, then loops are a good way to parse through lots of samples at once. I like to add a counter, so that I can keep track of where the script is at when I send it to a HPC.\nTo add a counter, you first must set the counter at one: COUNTER=1, then you will want to get the total number of samples to be processed: TOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l). With those now defined you can start the loop. In this example the loop is utilising the path ${DIRECTORY} and searching for files containing the suffix \\*_R1.fastq.gz. This is done to capture the sample ID by using basename to remove the path and suffic of the R1 file.\n\nCOUNTER=0\nTOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l); COUNTER=1\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n···\n\nWith the loop open, we can calculate the remaining number of samples to process: REMAINING=$((TOTAL - COUNTER)). Its important to know that shell can only perform simple mathematics, so bear this in mind when using its calculator functions. Then an echo -e is used to report which sample number the loop is on, and how many are remaining:\n\n···\necho -e \"   Sample: ${ID}  [${COUNTER}/${TOTAL}; ${REMAINING} remaining]\n                R1: ${DIRECTORY}/${ID}_R1.fastq.gz\n                R2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n···\n\nIn this example a tool called TB-Profiler is running on the R1 and R2 FASTQ files, utilising the \\${ID} variable defined at the start of the loop for each sample. With the main function defined, we have to remember to increase the counter by 1, so that it increases with the loop to the next file: COUNTER=$((COUNTER + 1)).\nIn all this might look something like this:\n\nCOUNTER=1 # start the counter\nTOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l) # get the total\n\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n    ID=$(basename \"${file}\" _R1.fastq.gz)\n    \n    REMAINING=$((TOTAL - COUNTER)) # Calculate remaining samples\n\n    # Display sample information with the counter\n    echo -e \"   Sample: ${ID}  [${COUNTER}/${TOTAL}; ${REMAINING} remaining]\n                R1: ${DIRECTORY}/${ID}_R1.fastq.gz\n                R2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n\n    # Run the profiling command\n    tb-profiler profile -1 ${DIRECTORY}/${ID}_R1.fastq.gz \\\n                        -2 ${DIRECTORY}/${ID}_R2.fastq.gz \\\n                        -t 4 -p ${ID} --txt\n        \n    COUNTER=$((COUNTER + 1)) # Increment the counter\ndone\n\nYou can even combine the loop with the if statement:\n\nCOUNTER=0 # start the counter at zero\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n    ID=$(basename \"${file}\" _R1.fastq.gz)\n    # Calculate remaining samples\n    REMAINING=$((TOTAL - COUNTER))\n\n    # If argument to check that the TB_profile hasnt already been run:\n    if [[ ! -f ${TBPROF_DIR}/results/${ID}.results.txt ]]; then\n        # Display sample information with the counter\n        echo -e \"${cyan}\\tSample: ${ID}\\t\\t[$COUNTER/$TOTAL; $REMAINING remaining]\\n\\t\\tR1: ${DIRECTORY}/${ID}_R1.fastq.gz\\n\\t\\tR2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n        echo -e \"${nocolor}\"\n        # Run the profiling command\n        tb-profiler profile -1 ${DIRECTORY}/${ID}_R1.fastq.gz -2 ${DIRECTORY}/${ID}_R2.fastq.gz -t ${NTHREADS} -p ${ID} --txt\n        # Increment the counter\n        COUNTER=$((COUNTER + 1))\n    elif [[ -f ${TBPROF_DIR}/results/${ID}.results.txt ]]; then\n        echo -e \"${red}\\t${TBPROF_DIR}/results/${ID}.results.txt exists, skipping: ${ID}\\t\\t[$COUNTER/$TOTAL; $REMAINING remaining]\"\n        COUNTER=$((COUNTER + 1))\n    fi\ndone\n\n\nPart 2 ⋘ Part 3 ⋙ Part 4"
  }
]