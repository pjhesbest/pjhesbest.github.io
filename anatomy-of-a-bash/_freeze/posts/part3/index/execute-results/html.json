{
  "hash": "b45fc4a78d36b62cf85f43813ad71fba",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Part 3: \"\nauthor: \"Poppy J Hesketh Best\"\ndate: \"2025-08-28\"\ncategories: [code, analysis, beginners, \"Anatomy of a BASH\", \"Guide\"]\nimage: \"profile.png\"\nexecute:\n  echo: true\n  eval: false\n  engine: knitr\n---\n\nHere we cover generating log files, and environment set up.\n\n**[Part 2]() ⋘ Part 3 ⋙ [Part 4]()**\n\n---\n\n## Generating logs-files\n\nLog files are invaluable for troubleshooting your scripts, or having a record of every run of your script (think of them as a lab book entry). You might not always keep all logs, but they are useful for looking back and remining yourself of what parameters the script was run using.\n\nFirst we want to create an accrate time-point in the format of YYYYMMDDHHMMSS, this might seem excessive, but considering you could be running this script multiple time making small adjustments, you want each logfile to be completely unique to not overwrite them. To achieve that we create a variable that contains the log file name. First create a variable called `${logfile}`, we use the default BASH function `date`, and specifiy the date format `'+%Y%m%d%H%M%S'`. We then add the suffix `.log` to the file name.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nlogfile=\"$(date '+%Y%m%d%H%M%S')\".log\n```\n:::\n\n\nNow with the logfile name defined as a variable (`${logfile}`), we redirect both standard output (`stdout`) and standard error (`stderr`) to a log file, while also displaying them on the terminal at the same time. Let's break it down step by step:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nexec > >( tee -a \"${logfile}\" ) 2>&1\n```\n:::\n\n\n- `exec` changes the behavior of stdout and stderr for the rest of the script, \n- `stdout` (`>`) is redirected into a process substitution: `>( tee -a \"${logfile}\" ) `. This sends the output to the `tee` command. \n- `tee` displays the output on the terminal (as usual) and appends it to the file `${logfile}`. \n- `2>&1` redirects stderr to the same destination as stdout, meaning that errors are also both displayed on the terminal and written to the log file.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nlogfile=\"$(date '+%Y%m%d%H%M%S')\".log\nexec > >( tee -a \"${logfile}\" ) 2>&1\n```\n:::\n\n\nWith that you will generate a output containing the standard outputs and the standard errors, which is great for troubleshooting your code. If you are routinely sending scripts to a HPC, the submission manager of that HPC more than likely generates log files, so you might not actually need this unless you want to hoard log files. \n\n# Environment setup\n\nNow that much of your script is defined for function, you can begin setting up the working environment to utilise your desired function. This might include defining paths to databases, or opening a conda enviroment. This is a very personalised process for your script and I wont cover mush, but will make a note about conda.\n\n### Conda\nWhen running conda in a HPC, you often have to configure your shell environment for conda. This command is typically required to properly initialize conda within a shell session, especially if the conda base environment isn't automatically loaded by default (e.g., when you don't have conda in your PATH at shell startup). So if you are struggling to get conda working in your HPC, this might be why.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\neval \"$(conda shell.bash hook)\"\nconda activate assembly-env\n```\n:::\n\n### Printing flag parameters\nIf you are going to be generating a log-file for a record of your analysis/data processing, I find it useful to print you very close to the start of the string a list of all the variables utilise in the flags, data paths, data-files used, ect. This might look like this\n\n\n::: {.cell}\n\n```{.bash .cell-code}\necho \"Running assembly.sh with the following parameters\nSampleID: \"${sample}\"\nR1: \"${forward}\"\nR2: \"${reverse}\"\nMinimum read length: ${min_rlen}\nMinimum contig length: ${min_clen}\n\"\n```\n:::\n\n\nAlternatively you could opt to print this out as a `CSV` file containing your run parameters. \nPersonalising your scripts is very simple, and can be easily modified to meet your experimental needs. \n\n\n::: {.cell}\n\n```{.bash .cell-code}\necho \"${logfile};${forward}:${reverse};${min_rlen};${min_clen}\" >> assembly.params.log\n```\n:::\n\n\n## Making a pretty script!\nIn the long matrix stream of text that can be printed to you terminal, it is often useful to have important text stand out. This could be a missing file, non-existent paths, location of output file...\n\nAdding color to you script can be definted very easily. First you must indicate the color of the text needs to change using: `\\033[`. This open up all subsequent text to a color change, BUT no color has been defined yet. To define a color your must add the approporiate ANSI escape codes (Table 1). For red text you would define it as: `\\033[31m`. All text that follows this will be green. To revert the color back to default (typically white in a console), you need to close the escape code with a `\\033[m`.\n\n**Table 1.** ANSI color codes and their corresponding colors.\n\n| code | color  |\n|------|--------|\n| 31m  | red    |\n| 32m  | green  |\n| 36m  | cyan   |\n| 36m  | purple |\n| 34m  | blue   |\n| 33m  | orange |\n\nA complete example might look something like this (if you are printing text to the console using the command `echo`, then you also have to add `-e` to enable interpretation of backslash escapes):\n\n\n::: {.cell}\n\n```{.bash .cell-code}\necho -e \"\\033[31m ERROR: something has gone very wrong because the text is all red!\\033[m\"\n```\n:::\n\n\nThis is quite a awkward to add and you can very easily miss a closure, and the the entirely of your console is colored red and you'll feel like you have made a terrible bloody mistake. A work around is to define the colors as a variables at the start of your script:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Define colors\ngreen='\\033[32m'; red='\\033[31m'; cyan='\\033[36m'; purple='\\033[35m'; nocolor='\\033[m'\n```\n:::\n\n\nThis alows you to change the text color as follows:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\necho -e \"${red}ERROR: something has gone very wrong because the text is all red!${nocolor}\"\n```\n:::\n\n\n## Adding a timestamp to the end of your script\n\nIf your optimising your scripts and trying to get an idea of run-time and computing resources its useful to print out the run time. This is easily done by first creating a timestamp at the  start and the end of your script, the  calculating the elapsed time, then printing the time.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/usr/bin/env bash\nstart_time=$(date +%s) # define start time\n\n#··············································#\n#··········· The rest of the script ···········#\n#··············································#\n\nfinish_time=$(date +%s) # define end time\n\n# Calculate the difference in seconds\nelapsed_time=$((finish_time - start_time))\n\n# Convert elapsed time to hours, minutes, and seconds\n((sec=elapsed_time%60, elapsed_time/=60, min=elapsed_time%60, hrs=elapsed_time/60))\ntimestamp=$(printf \"Total time taken - %d hours, %d minutes, and %d seconds.\" $hrs $min $sec)\necho $timestamp\n\n# Print the total runtime\necho -e \"\"\necho -e \"\"Total time taken: ${hrs}:${min}:${sec}\"\n\n```\n:::\n\n\nIf you are testing multiple computing resource allocation and different processing time, you might even consider printing a running table of your experiment along with your run parameters. For example, in the case of a a phylogeny script (`phylogeny`)\n\n::: {.cell}\n\n```{.bash .cell-code}\necho -e \"phylogeny.sh;${THREADS};${NBOOTSTRAPS};${NUMSEQUENCES};${hrs}:${min}:${sec}\" >> computing-time-test.csv\n```\n:::\n\n\n## `true`/`false` scenarios:\n\nPreviously the idea of having a true/false argument for dissabling the error correction of SPAdes, as an example. If you have included an argument for when `QCSPADes == true` and when `QCSPADes == false`.\n\nLike before we start the if statement with a scenario, which is `QCSPADes == true` (for when the -Q flag is uses), then we direct a specific action for that scenario, which is to performs SPADes using the assembly only features disabling the error correction. \n\n\n::: {.cell}\n\n```{.bash .cell-code}\n\nif [[ QCSPADes == true ]]; then \n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out --only-assembler\n···\n```\n:::\n\n\nThen we can continue the if statement to include an alternative scenario using an 'else if' (elif) statement, for for scenario when `QCSPADes == false`, to perform spades with default features. This is also the default functioning of the example script as we defined `Q)QCSPADes=false;;` at the start of the script.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n···\nelif [[ QCSPADes == false ]]; then\n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out\nfi\n```\n:::\n\nThe entire if argument will look as follows:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nif [[ QCSPADes == true ]]; then \n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out --only-assembler\nelif [[ QCSPADes == false ]]; then\n  spades.py -1 sample-1_R1.fastq.gz -2 sample-1_R2.fastq.gz -o sample-1_assembly-out\nfi\n```\n:::\n\n\n## Not running the script if the output of the tool already exists:\n\nIn the example [bin/tb-profiler_v1.sh](URL) you can see that there is an example to search a collated output from previous runs of this scrip before deciing to run the script or not. It uses an `if` statement to determine wether that particular sampleID exists the output, and if it does not (i.e. ) to run the script. But if the output does exists (i.e. ), the it skips that particuler sample.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n\n```\n:::\n\n\n## Forcing the script to run even if the output exists:\nIn a more sophisticated version of the script [bin/tb-profiler_v2.sh](URL) there is an additional flag to force the script to overright the previous output (i.e. ) using a flag of `-F`.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n\n```\n:::\n\n\n## Adding a counter if processing multiple samples\nIf a single process is quick and not computationaly demanding, then loops are a good way to parse through lots of samples at once. I like to add a counter, so that I can keep track of where the script is at when I send it to a HPC.\n\nTo add a counter, you first must set the counter at one: `COUNTER=1`, then you will want to get the total number of samples to be processed: `TOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l)`. With those now defined you can start the loop. In this example the loop is utilising the path `${DIRECTORY}` and searching for files containing the suffix `\\*_R1.fastq.gz`. This is done to capture the sample ID by using `basename` to remove the path and suffic of the R1 file.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nCOUNTER=0\nTOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l); COUNTER=1\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n···\n```\n:::\n\nWith the loop open, we can calculate the remaining number of samples to process: `REMAINING=$((TOTAL - COUNTER))`. Its important to know that shell can only perform simple mathematics, so bear this in mind when using its calculator functions. Then an `echo -e` is used to report which sample number the loop is on, and how many are remaining: \n\n\n::: {.cell}\n\n```{.bash .cell-code}\n···\necho -e \"   Sample: ${ID}  [${COUNTER}/${TOTAL}; ${REMAINING} remaining]\n                R1: ${DIRECTORY}/${ID}_R1.fastq.gz\n                R2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n···\n```\n:::\n\n\nIn this example a tool called TB-Profiler is running on the R1 and R2 FASTQ files, utilising the `\\${ID}` variable defined at the start of the loop for each sample. With the main function defined, we have to remember to increase the counter by 1, so that it increases with the loop to the next file: `COUNTER=$((COUNTER + 1))`. \n\nIn all this might look something like this:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nCOUNTER=1 # start the counter\nTOTAL=$(ls ${DIRECTORY}/*R1.fastq.gz | wc -l) # get the total\n\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n    ID=$(basename \"${file}\" _R1.fastq.gz)\n    \n    REMAINING=$((TOTAL - COUNTER)) # Calculate remaining samples\n\n    # Display sample information with the counter\n    echo -e \"   Sample: ${ID}  [${COUNTER}/${TOTAL}; ${REMAINING} remaining]\n                R1: ${DIRECTORY}/${ID}_R1.fastq.gz\n                R2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n\n    # Run the profiling command\n    tb-profiler profile -1 ${DIRECTORY}/${ID}_R1.fastq.gz \\\n                        -2 ${DIRECTORY}/${ID}_R2.fastq.gz \\\n                        -t 4 -p ${ID} --txt\n        \n    COUNTER=$((COUNTER + 1)) # Increment the counter\ndone\n```\n:::\n\n\nYou can even combine the loop with the `if` statement:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nCOUNTER=0 # start the counter at zero\nfor file in ${DIRECTORY}/*R1.fastq.gz; do\n    ID=$(basename \"${file}\" _R1.fastq.gz)\n    # Calculate remaining samples\n    REMAINING=$((TOTAL - COUNTER))\n\n    # If argument to check that the TB_profile hasnt already been run:\n    if [[ ! -f ${TBPROF_DIR}/results/${ID}.results.txt ]]; then\n        # Display sample information with the counter\n        echo -e \"${cyan}\\tSample: ${ID}\\t\\t[$COUNTER/$TOTAL; $REMAINING remaining]\\n\\t\\tR1: ${DIRECTORY}/${ID}_R1.fastq.gz\\n\\t\\tR2: ${DIRECTORY}/${ID}_R2.fastq.gz\"\n        echo -e \"${nocolor}\"\n        # Run the profiling command\n        tb-profiler profile -1 ${DIRECTORY}/${ID}_R1.fastq.gz -2 ${DIRECTORY}/${ID}_R2.fastq.gz -t ${NTHREADS} -p ${ID} --txt\n        # Increment the counter\n        COUNTER=$((COUNTER + 1))\n    elif [[ -f ${TBPROF_DIR}/results/${ID}.results.txt ]]; then\n        echo -e \"${red}\\t${TBPROF_DIR}/results/${ID}.results.txt exists, skipping: ${ID}\\t\\t[$COUNTER/$TOTAL; $REMAINING remaining]\"\n        COUNTER=$((COUNTER + 1))\n    fi\ndone\n```\n:::\n\n\n## Submitting modular scripts to HCP\n\nDepending on your computing cluster server, you will be able to directly submit these scripts given that you provide sufficient information in your code to run with the necessary computing resources.\n\n**SunGrid system**\nFor a SunGrid system you have to use `qsub` as your main command for submitting scripts. Because of how these scripts were writen, there are not cluster specific parameters defined within the script, so they can be defined with the script submission command\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nqsub \n```\n:::\n\n\n\n## Example scripts\nHere are some examples of scripts, you can also find these scripts in in `bin/`. Take note how these scripts make use of all that has been discussed in this guide \n![image](Figure3.png)\n\n![image](Figure4.png)\n\n---\n\n**[Part 2]() ⋘ Part 3 ⋙ [Part 4]()**",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}